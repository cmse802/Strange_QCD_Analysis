{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using Statistical Models in High Energy Physics</center>\n",
    "\n",
    "<center>by Nick Juliano</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical techniques are extremely common in HEP, a field that often requires detailed data analysis. When we obtain fits for predictive models, we rely heavily on error estimates to decide if our results agree with experimental data. In lattice QCD, we often use jackknifed data samples and so our error takes the form of jackknifed estimates. We calculate the $i$th subsample of $N$ data points by computing a statistic using all but the $i$th data point. For example, to calculate the jackknifed mean:\n",
    "$$\n",
    "    \\bar{x_i} = \\frac{1}{n-1} \\sum_{j \\neq i}^N x_j.\n",
    "$$\n",
    "\n",
    "The average of these subsamples is our jackknifed parameter, which is what we use to compute the modified variance. The jackknife method is useful in HEP because we often have no theoretical expectation for the error. Experimentalists in this field are actively working to reduce the uncertainties in measured quantities (like particle distributions). Jackknife resampling is an all-purpose method to estimate error, so it is well-suited to these types of predictive problems.\n",
    "\n",
    "One of the central topics in lattice QCD (and the one that I am currently focused on) is the determination of parton distribution functions (PDFs), which is essentially an exercise in statistical modeling. On the theoretical side, we determine PDFs through a variety of fitting techniques. For example, we may train a neural network to predict the functional form of a PDF for a quark species. To evaluate each fit, we often look to the $\\chi^2$ distribution, where we compare the model's prediction to the training data. The $\\chi^2$ test comes immediately from many statistics packages, but it is rarely the sole statistic that we use-- it is more effective to rule out a bad fit than to conclude that a fit is good.\n",
    "\n",
    "On the topic of training machine learning models, we also use statistical techniques to process training data. One problem that we have is overfitting; when preparing training data, we tread a fine line between giving the model flexibility and ensuring that the predictions will eventually agree with experiments. A question in our research group has been what functional forms to train models with. We know the behavior of some PDFs from prior research, but training on only high precision data will likely lead to poor predictions for unknown PDFs (a consequence of overfitting). To combat this, I am interested in applying Gaussian smearing to a set of fairly general training data. Gaussian smearing has different applications in statistical fields, but in this context, it would give me a way to add normalized noise to the training data with the hope of improving model predictions. I do not know how to implement Gaussian smearing yet, but I am interested to see how it affects our PDF fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "1. De, Harindranath, Maiti. Investigation of Lattice QCD with Wilson fermions with Gaussian Smearing, 2007, https://arxiv.org/abs/0712.4354\n",
    "2. Wei Chen et al. Lattice Calculation of Parton Distribution Function from LaMET at Physical Pion Mass with Large Nucleon Momentum, 2018, https://arxiv.org/abs/1803.04393"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
